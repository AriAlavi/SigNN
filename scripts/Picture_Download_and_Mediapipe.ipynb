{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Picture Download and Mediapipe",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp5kDhCnQ3R8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "426943a2-5f59-4331-844f-a8027d36308b"
      },
      "source": [
        "# Compile mediapipe\n",
        "import os\n",
        "import json\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "class GoogleDriveDatabase:\n",
        "  def __init__(self, drive, DATABASE_GID:str):\n",
        "    assert isinstance(DATABASE_GID, str)\n",
        "    self.folders = {}\n",
        "    self.drive = drive\n",
        "    self.database_gid = DATABASE_GID\n",
        "    folder_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(DATABASE_GID)}).GetList()\n",
        "    for file in folder_list:\n",
        "      if file['mimeType'] == \"application/vnd.google-apps.folder\":\n",
        "        self.folders[file['title']] = file['id']\n",
        "    self.createFolder(\"TRASH\")\n",
        "    print(\"{} folders loaded\".format(len(self.folders.keys())))\n",
        "  def upload(self, filename, character, fileType):\n",
        "    assert isinstance(filename, str)\n",
        "    assert isinstance(character, str)\n",
        "    assert isinstance(fileType, str)\n",
        "    FILETYPE_MIME_MAP = {\n",
        "        \"jpeg\" : \"image/jpeg\",\n",
        "        \"json\" : \"application/json\",\n",
        "        \"zip\" : \"application/zip\"\n",
        "    }\n",
        "    assert fileType in FILETYPE_MIME_MAP.keys(), \"fileType must be one of the following: {}\".format(fileType)\n",
        "    assert os.path.isfile(filename), \"{} does not exist as a file\".format(filename)\n",
        "    assert self.checkFolder(character), \"{} is not a valid character. Pick from list: \\n{}\".format(character, tuple(self.folders.keys()))\n",
        "    file = self.drive.CreateFile({\n",
        "        \"title\" :  os.path.split(filename)[1],\n",
        "        \"mimeType\" : FILETYPE_MIME_MAP[fileType],\n",
        "        \"parents\" : [{\"id\" : self.folders[character]}]\n",
        "    })\n",
        "    file.SetContentFile(filename)\n",
        "    file.Upload()\n",
        "    os.remove(filename)\n",
        "    print(\"uploaded and deleted {}\".format(filename))\n",
        "\n",
        "  @staticmethod\n",
        "  def allImagesToJPG(fileName) -> str:\n",
        "    assert isinstance(fileName, str)\n",
        "    assert any(extension for extension in GoogleDriveDatabase.IMAGE_EXTENSIONS()), \"{} is not a valid image\".format(fileName)\n",
        "    if \".jpg\" in fileName:\n",
        "      return fileName\n",
        "    return fileName.split(\".\")[0] + \".jpg\"\n",
        "    \n",
        "  @staticmethod\n",
        "  def IMAGE_EXTENSIONS() -> list:\n",
        "    return [\".jpg\", \".jpeg\", \".png\"]\n",
        "\n",
        "  @staticmethod\n",
        "  def FILE_EXTENSIONS() -> list:\n",
        "    return [\".jpg\", \".jpeg\", \".png\", \".zip\", \".json\"]\n",
        "\n",
        "  def getFiles(self, character) -> list:\n",
        "    return [\n",
        "      x for x in self.drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(self.folders[character])}).GetList()\n",
        "      if x['mimeType'] != \"application/vnd.google-apps.folder\" \n",
        "    ]\n",
        "  def download_file_name(self, file_name):\n",
        "      if not any(extension in file_name for extension in GoogleDriveDatabase.FILE_EXTENSIONS()):\n",
        "        file_name += \".jpg\"\n",
        "      return GoogleDriveDatabase.allImagesToJPG(file_name)\n",
        "\n",
        "  def download_file(self, file, folder:str, **kwargs) -> str:\n",
        "      check_already_exist = kwargs.get(\"check_local\", False)\n",
        "      file_name = os.path.join(folder, file['title'])\n",
        "      file_name = self.download_file_name(file_name)\n",
        "      if check_already_exist:\n",
        "        local_files = [os.path.join(folder, file) for file in os.listdir(folder)]\n",
        "        if file_name in local_files:\n",
        "          return file_name\n",
        "      file.GetContentFile(file_name)\n",
        "      print(\"downloaded\", file_name)\n",
        "      return file_name\n",
        "\n",
        "  def download(self, character:str,folder:str):\n",
        "    file_list = self.getFiles(character)\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    returnlist = []\n",
        "    \n",
        "    for file in file_list:\n",
        "      returnlist.append(self.download_file(file, folder, check_local=True))\n",
        "    return tuple(returnlist)\n",
        "\n",
        "  def checkFolder(self, name):\n",
        "    assert isinstance(name, str)\n",
        "    return name in self.folders.keys()\n",
        "\n",
        "  def createFolder(self, name, exist_ok=True):\n",
        "    if self.checkFolder(name):\n",
        "      if exist_ok:\n",
        "        return True\n",
        "      raise Exception(\"Folder {} already exists\".format(name))\n",
        "    self.drive.CreateFile({\n",
        "        \"title\" : name,\n",
        "        \"mimeType\" : \"application/vnd.google-apps.folder\",\n",
        "        \"parents\" : [{\"id\" : self.database_gid}]\n",
        "    }).Upload()\n",
        "  def move_file(self, file_obj, newFolder):\n",
        "    assert isinstance(newFolder, str)\n",
        "    assert self.checkFolder(newFolder)\n",
        "    files = self.drive.auth.service.files()\n",
        "    file = files.get(fileId=file_obj['id'], fields ='parents').execute()\n",
        "    prev_parents = ','.join(p['id'] for p in file.get('parents'))\n",
        "    file = files.update(\n",
        "        fileId = file_obj['id'],\n",
        "        addParents = self.folders[newFolder],\n",
        "        removeParents = prev_parents,\n",
        "        fields = 'id, parents',\n",
        "    ).execute()\n",
        "    return file\n",
        "  def trash(self, file_obj):\n",
        "    self.move_file(file_obj, \"TRASH\")\n",
        "    print(\"Sent {} to trash\".format(file_obj['title']))\n",
        "\n",
        "auth.authenticate_user() # Google auth stuff, make sure to sign in with your ucsb account\n",
        "gauth = GoogleAuth() # Google auth stuff\n",
        "gauth.credentials = GoogleCredentials.get_application_default() # Google auth stuff\n",
        "drive = GoogleDrive(gauth) # Google auth stuff\n",
        "\n",
        "\n",
        "DOWNLOAD_DATABASE = \"1RqiJwO6i1KJx54tRC3QJcHWjiooCLJNC\"\n",
        "UPLOAD_DATABASE = \"1wYWIJFfm2la6KrimpC5mWFBpyITcUT6U\"\n",
        "download_database = GoogleDriveDatabase(drive, DOWNLOAD_DATABASE)\n",
        "upload_database = GoogleDriveDatabase(drive,UPLOAD_DATABASE)\n",
        "CHARACTERS = (\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\")\n",
        "[upload_database.createFolder(x,exist_ok=True) for x in CHARACTERS]\n",
        "git_repo_url = 'https://github.com/AriAlavi/SigNN.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sudo apt install curl\n",
        "  !curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\n",
        "  !echo \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\n",
        "  !sudo apt update && sudo apt install bazel-3.3.0\n",
        "  !sudo apt-get install libopencv-core-dev libopencv-highgui-dev \\\n",
        "                        libopencv-calib3d-dev libopencv-features2d-dev \\\n",
        "                        libopencv-imgproc-dev libopencv-video-dev\n",
        "  !cd {project_name} && bazel-3.3.0 build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/multi_hand_tracking:multi_hand_tracking_cpu \n",
        "else:\n",
        "  !git config --global user.email \"none@gmail.com\"\n",
        "  !git config --global user.name \"Google colab\"\n",
        "  !cd {project_name} && git stash\n",
        "  !cd {project_name} && git pull\n",
        "  !cd {project_name} && bazel-3.3.0 build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/multi_hand_tracking:multi_hand_tracking_cpu "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26 folders loaded\n",
            "25 folders loaded\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.58.0-2ubuntu3.10).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3199  100  3199    0     0  15681      0 --:--:-- --:--:-- --:--:-- 15681\n",
            "OK\n",
            "deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [95.7 kB]\n",
            "Get:3 https://storage.googleapis.com/bazel-apt stable InRelease [2,256 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:12 https://storage.googleapis.com/bazel-apt stable/jdk1.8 amd64 Packages [4,567 B]\n",
            "Get:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [47.5 kB]\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [10.1 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [897 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [43.0 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [116 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,089 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [132 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,384 kB]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,864 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,425 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.7 kB]\n",
            "Get:28 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [900 kB]\n",
            "Fetched 8,332 kB in 6s (1,332 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "72 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  bash-completion\n",
            "The following NEW packages will be installed:\n",
            "  bazel-3.3.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 72 not upgraded.\n",
            "Need to get 43.2 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://storage.googleapis.com/bazel-apt stable/jdk1.8 amd64 bazel-3.3.0 amd64 3.3.0 [43.2 MB]\n",
            "Fetched 43.2 MB in 4s (12.0 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package bazel-3.3.0.\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../bazel-3.3.0_3.3.0_amd64.deb ...\n",
            "Unpacking bazel-3.3.0 (3.3.0) ...\n",
            "Setting up bazel-3.3.0 (3.3.0) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopencv-calib3d-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "libopencv-calib3d-dev set to manually installed.\n",
            "libopencv-core-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "libopencv-core-dev set to manually installed.\n",
            "libopencv-features2d-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "libopencv-features2d-dev set to manually installed.\n",
            "libopencv-highgui-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "libopencv-highgui-dev set to manually installed.\n",
            "libopencv-imgproc-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "libopencv-imgproc-dev set to manually installed.\n",
            "libopencv-video-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
            "libopencv-video-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 72 not upgraded.\n",
            "Extracting Bazel installation...\n",
            "Starting local Bazel server and connecting to it...\n",
            "WARNING: ignoring LD_PRELOAD in environment.\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @bazel_skylib; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @bazel_skylib; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @bazel_skylib; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @build_bazel_rules_apple; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @build_bazel_rules_apple; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @build_bazel_rules_apple; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @build_bazel_rules_swift; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @build_bazel_apple_support; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @rules_jvm_external; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @rules_jvm_external; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @rules_jvm_external; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @rules_jvm_external; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @rules_jvm_external; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @rules_jvm_external; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @rules_jvm_external; fetching\n",
            "    Fetching ...ternal; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 4s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 4s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 4s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 4s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 5s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 5s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 5s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 5s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 6s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 6s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 6s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 6s\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 7s\n",
            "    Fetching ...orflow; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "1dcd7692847de0d8c695/external/org_tensorflow/7c09d15f9fcc14343343c247ebf5b8e0a\\\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 8s\n",
            "    Fetching ...orflow; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "1dcd7692847de0d8c695/external/org_tensorflow/7c09d15f9fcc14343343c247ebf5b8e0a\\\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 9s\n",
            "    Fetching ...orflow; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "1dcd7692847de0d8c695/external/org_tensorflow/7c09d15f9fcc14343343c247ebf5b8e0a\\\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @org_tensorflow; fetching 10s\n",
            "    Fetching ...orflow; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "1dcd7692847de0d8c695/external/org_tensorflow/7c09d15f9fcc14343343c247ebf5b8e0a\\\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[35mWARNING: \u001b[0mDownload from https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/7c09d15f9fcc14343343c247ebf5b8e0afe3e4aa.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "\u001b[32mLoading:\u001b[0m 0 packages loaded\n",
            "    Fetching @io_bazel_rules_closure; fetching\n",
            "\u001b[35mWARNING: \u001b[0mDownload from http://mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/cf1e44edb908e9616030cc83d085989b8e6cd6df.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (1 packages loaded, 0 targets configured)\n",
            "    currently loading: @bazel_tools//tools/cpp\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (1 packages loaded, 0 targets configured)\n",
            "    currently loading: @bazel_tools//tools/cpp\n",
            "    Fetching @rules_cc; fetching\n",
            "\u001b[32mINFO: \u001b[0mSHA256 (https://github.com/bazelbuild/rules_cc/archive/master.zip) = 2a34fa56d923f774409d23720e60ddf6536e88622d000e6925f7cebbad65e281\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (1 packages loaded, 0 targets configured)\n",
            "    currently loading: @bazel_tools//tools/cpp\n",
            "\u001b[33mDEBUG: \u001b[0mRule 'rules_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = \"2a34fa56d923f774409d23720e60ddf6536e88622d000e6925f7cebbad65e281\"\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (1 packages loaded, 0 targets configured)\n",
            "\u001b[33mDEBUG: \u001b[0mRepository rules_cc instantiated at:\n",
            "  no stack (--record_rule_instantiation_callstack not enabled)\n",
            "Repository rule http_archive defined at:\n",
            "  /root/.cache/bazel/_bazel_root/6b05d56d57241dcd7692847de0d8c695/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (1 packages loaded, 0 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (8 packages loaded, 11 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (10 packages loaded, 11 targets configured)\n",
            "    currently loading: @bazel_tools//tools/jdk\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (16 packages loaded, 24 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (16 packages loaded, 24 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (16 packages loaded, 24 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (16 packages loaded, 24 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (18 packages loaded, 44 targets configured)\n",
            "    currently loading: mediapipe/framework ... (2 packages)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (19 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (2 packages)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "    Fetching @com_google_protobuf; fetching\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "    Fetching @com_google_protobuf; fetching\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "    Fetching @com_google_protobuf; fetching\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "    Fetching @com_google_protobuf; fetching\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "    Fetching @com_google_protobuf; fetching\n",
            "    Fetching ...otobuf; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (21 packages loaded, 56 targets configured)\n",
            "    currently loading: mediapipe/framework ... (3 packages)\n",
            "    Fetching @rules_python; fetching\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (23 packages loaded, 56 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (27 packages loaded, 73 targets configured)\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (27 packages loaded, 78 targets configured)\n",
            "    currently loading: @com_google_protobuf//\n",
            "    Fetching @com_google_absl; fetching\n",
            "    Fetching @rules_proto; fetching\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (27 packages loaded, 78 targets configured)\n",
            "    currently loading: @com_google_protobuf//\n",
            "    Fetching @com_google_absl; fetching\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (28 packages loaded, 78 targets configured)\n",
            "    Fetching @com_google_absl; fetching\n",
            "    Fetching ...e_absl; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (29 packages loaded, 78 targets configured)\n",
            "    currently loading: mediapipe/calculators/core\n",
            "    Fetching @com_google_absl; fetching\n",
            "    Fetching ...e_absl; Extracting /root/.cache/bazel/_bazel_root/6b05d56d5724\\\n",
            "\u001b[32mAnalyzing:\u001b[0m target //mediapipe/examples/desktop/multi_hand_tracking:multi_hand_\\\n",
            "tracking_cpu (36 packages loaded, 90 targets configured)\n",
            "    currently loading: @com_google_absl//absl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW2QIKDRIBtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "import uuid\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "BASE_RESOLUTION = (1920, 1920)\n",
        "\n",
        "def createVideoFromDirectory(directory, output_path):\n",
        "  assert isinstance(directory, str)\n",
        "  assert isinstance(output_path, str)\n",
        "  !ffmpeg -framerate 1 -pattern_type glob -i \"{directory}/*.jpg\" -c:v libx264 -r 1 -pix_fmt yuv420p {output_path} -loglevel error -y\n",
        "  return output_path\n",
        "\n",
        "def getImagesInFolder(absolutePath):\n",
        "    assert isinstance(absolutePath, str)\n",
        "    image_names = [os.path.join(absolutePath, img) for img in os.listdir(absolutePath) if img.endswith(\".jpg\") or img.endswith(\".jpeg\") or img.endswith(\".png\")]\n",
        "    return image_names\n",
        "  \n",
        "def runMediapipe(input_video):\n",
        "  assert os.path.isfile(input_video), \"{} does not exist\".format(input_video)\n",
        "  !cd SigNN && sudo GLOG_logtostderr=0 bazel-bin/mediapipe/examples/desktop/multi_hand_tracking/multi_hand_tracking_cpu --calculator_graph_config_file=mediapipe/graphs/hand_tracking/multi_hand_tracking_desktop_logger.pbtxt --input_video_path={input_video} --render_video=false\n",
        "\n",
        "def modifyMediapipeLoggerOutput(path, filename, mediapipe_directory):\n",
        "  assert isinstance(path, str)\n",
        "  assert isinstance(filename, str)\n",
        "  assert filename.split(\".\")[-1] == \"json\"\n",
        "  assert isinstance(mediapipe_directory, str)\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "  pbtxt_name = os.path.join(mediapipe_directory, \"mediapipe/graphs/hand_tracking/multi_hand_tracking_desktop_logger.pbtxt\")\n",
        "  pbtxt_file = open(pbtxt_name, \"r+b\")\n",
        "  pbtxt_content = pbtxt_file.readlines()\n",
        "  line_count = 0\n",
        "  for line in pbtxt_content:\n",
        "    if \"CoordinateLoggerCalculatorOptions\" in str(line):\n",
        "      pbtxt_content[line_count + 2] = '      logger_path: \"{}\"\\n'.format(path).encode()\n",
        "      pbtxt_content[line_count + 3] = '      filename: \"{}\"\\n'.format(filename).encode()\n",
        "      break\n",
        "    line_count += 1\n",
        "  pbtxt_file.seek(0)\n",
        "  pbtxt_file.truncate()\n",
        "  pbtxt_file.write(''.join([x.decode() for x in pbtxt_content]).encode())\n",
        "  pbtxt_file.close()\n",
        "\n",
        "class InvalidMediapipeOutput(Exception):\n",
        "  pass\n",
        "\n",
        "def Mediapipe(input_directory, output_directory):\n",
        "  VIDEO_NAME = \"out.mp4\"\n",
        "  os.makedirs(input_directory, exist_ok=True)\n",
        "  os.makedirs(output_directory, exist_ok=True)\n",
        "  print(\"Setting mediapipe logger output\")\n",
        "  modifyMediapipeLoggerOutput(output_directory, \"output.json\", \"/content/SigNN/\")\n",
        "  video_path = os.path.join(input_directory, VIDEO_NAME)\n",
        "  OUTPUT_FILE = os.path.join(output_directory, \"output.json\")\n",
        "  image_raw_names = getImagesInFolder(input_directory)\n",
        "  print(\"Creating videos from images\")\n",
        "  createVideoFromDirectory(input_directory, video_path)\n",
        "  print(\"Running mediapipe on {} images\".format(len(getImagesInFolder(input_directory))))\n",
        "  runMediapipe(video_path)\n",
        "  output_file_obj = open(OUTPUT_FILE, \"r\")\n",
        "  output_file_single_list = json.load(output_file_obj)\n",
        "  output_file_obj.close()\n",
        "  if len(output_file_single_list) != len(image_raw_names):\n",
        "    raise InvalidMediapipeOutput(\"There were {} coordinates created but there were {} images fed in\".format(len(output_file_single_list), len(image_raw_names)))\n",
        "  i = 0\n",
        "  json_files = []\n",
        "  for coordinates in output_file_single_list:\n",
        "    image_full_path = image_raw_names[i]\n",
        "    image_name = os.path.split(image_full_path)[1]\n",
        "    json_name = image_name.split(\".\")[0] + \".json\"\n",
        "    json_path = os.path.join(output_directory, json_name)\n",
        "    json_file = open(json_path, \"w\")\n",
        "    json.dump(coordinates, json_file)\n",
        "    json_file.close()\n",
        "    json_files.append(json_path)\n",
        "    i += 1\n",
        "  !rm -rf {input_directory}\n",
        "  return json_files\n",
        "\n",
        "\n",
        "\n",
        "# def download_characters(): # Only downloads characters\n",
        "#   BASE_DIR = \"images/\"\n",
        "#   results = {}\n",
        "#   for c in CHARACTERS:\n",
        "#     results[c] = download_database.download(c, os.path.join(BASE_DIR, c + \"/\"))\n",
        "#     print(\"{} pictures downloaded for {}\".format(len(results[c]), c))\n",
        "#   return results\n",
        "    \n",
        "def DownloadCharacterAndUploadJson(character, max_count, **kwargs):\n",
        "  assert isinstance(character, str)\n",
        "  assert isinstance(max_count, int)\n",
        "  ACCEPT_KWARGS = [\"only_get_pictures\", \"prefed_pictures\"]\n",
        "  only_get_pictures = kwargs.get(\"only_get_pictures\", False)\n",
        "  prefed_pictures = kwargs.get(\"prefed_pictures\", [])\n",
        "  assert isinstance(only_get_pictures, bool)\n",
        "  assert isinstance(prefed_pictures, list)\n",
        "  for x in kwargs.keys():\n",
        "    if x not in ACCEPT_KWARGS:\n",
        "      raise Exception(\"{} is not a valid kwargs argument\".format(x))\n",
        "\n",
        "  already_analyzed_names = set() # Names of all already analyzed items\n",
        "  pictures_to_analyze = [] # Pictures that yet do not have a json file\n",
        "  pictures_to_analyze_names = [] # Names of pictures that yet do not have a json file\n",
        "  BASE_FOLDER = \"images/\" # Directory where all images are\n",
        "  character_folder = os.path.join(BASE_FOLDER, character + \"/\") # Folder in which the images for this character are in (i.e. images/A)\n",
        "  os.makedirs(character_folder, exist_ok=True) # Make the character folder if it doesn't exist\n",
        "  if only_get_pictures: # Uneeded unless specifically asked for in kwargs\n",
        "    pictures_to_analyze_references = [] # References of all pictures to analyze\n",
        "  strip_name = lambda x: x.split(\"/\")[-1].split(\".\")[0] # Function to remove extensions (i.e. .json .jpg) from filenames\n",
        "\n",
        "  if not prefed_pictures:\n",
        "    character_jsons_original = upload_database.getFiles(character) # All .json files created for this character\n",
        "    character_jsons_names = [strip_name(x['title']) for x in character_jsons_original] # Name of all .json files already created for this character\n",
        "    picture_references = download_database.getFiles(character) # All current pictures for this character\n",
        "    picture_reference_names = [strip_name(x['title']) for x in picture_references] # Name of all pictures\n",
        "    print(\"There are {} images for character {}\".format(len(picture_references), character))\n",
        "    print(\"There are {} json files already made for character {}\".format(len(character_jsons_names), character))\n",
        "    for json_original in character_jsons_original: # For every json file on the google drive...\n",
        "      if strip_name(json_original['title']) not in picture_reference_names: # ...if the json file is not in the list of pictures...\n",
        "        print(\"{} deleted because no image was found to relate to it\".format(json_original['title']))\n",
        "        upload_database.trash(json_original) # ...then delete the file\n",
        "  else:\n",
        "    picture_references = prefed_pictures\n",
        "    print(\"{} pictures pre-fed\".format(len(prefed_pictures)))\n",
        "\n",
        "  for picture in picture_references: # For every picture on google drive...\n",
        "    picture_name = strip_name(picture['title'])\n",
        "    if not prefed_pictures and picture_name in character_jsons_names: # ...if that picture already has a json uploaded...\n",
        "      already_analyzed_names.add(download_database.download_file_name(picture['title'])) # ...add its name to the list of already analyzed pictures\n",
        "      continue\n",
        "    elif max_count != 0:\n",
        "      pictures_to_analyze.append(download_database.download_file(picture, character_folder, check_local=True)) # ...else download it\n",
        "      pictures_to_analyze_names.append(picture_name)\n",
        "      if only_get_pictures:\n",
        "        pictures_to_analyze_references.append(picture)\n",
        "      max_count -= 1\n",
        "    else:\n",
        "      break\n",
        "  if only_get_pictures:\n",
        "    return pictures_to_analyze_references\n",
        "\n",
        "  local_duplicates = [os.remove(os.path.join(character_folder, x)) for x in os.listdir(character_folder) if os.path.isfile(os.path.join(character_folder, x)) and strip_name(x) not in pictures_to_analyze_names] # if a picture is not supposed to be analyzed list and exists in the local directory, delete it so that mediapipe doesn't waste time running it\n",
        "  print(\"{} extra local files detected\".format(len(local_duplicates)))\n",
        "  if len(os.listdir(character_folder)) == 0:\n",
        "    print(\"{} has no images to process\".format(character))\n",
        "    return []\n",
        "\n",
        "  BASE_IMAGES_DIR = \"/content/images/\"\n",
        "  BASE_JSON_DIR = \"/content/json/\"\n",
        "  json_created = Mediapipe(os.path.join(BASE_IMAGES_DIR, character), os.path.join(BASE_JSON_DIR, character)) # runs mediapipe on all images in a given path and outputs each image to an output path\n",
        "  for json_file in json_created: # For every local json file created...\n",
        "    upload_database.upload(json_file, character, \"json\") #...upload it to google drive\n",
        "  return pictures_to_analyze\n",
        "\n",
        "def DownloadCharacterAndUploadJsonRecursively(character, max_count=100, prefed_set=[]):\n",
        "  print(\"\\n\")\n",
        "  if len(prefed_set) == 1: # Recursive base case, if there is one pictures that is causing problems\n",
        "    download_database.trash(prefed_set[0]) # Then delete it\n",
        "    return [] # There is no valid picture\n",
        "  error = None\n",
        "  try:\n",
        "    return DownloadCharacterAndUploadJson(character, max_count, prefed_pictures=prefed_set) # Try to run mediaipe on the set\n",
        "  except InvalidMediapipeOutput as e: # If there is a picture casuing problems\n",
        "    error = e\n",
        "  except:\n",
        "    time.sleep(60 * 5)\n",
        "    gauth.Refresh()\n",
        "    return DownloadCharacterAndUploadJson(character, max_count, prefed_pictures=prefed_set)\n",
        "  if len(prefed_set) == 0: # And there is no history of pictures casuing problems\n",
        "    prefed_set = DownloadCharacterAndUploadJson(character, max_count, only_get_pictures=True) # Get a list of the suspect pictures\n",
        "  print(\"Problems with mediapipe pictures: {}. Splitting list of {} in half.\".format(error, len(prefed_set)))\n",
        "  half_count = int(len(prefed_set) / 2)\n",
        "  first_half = prefed_set[:half_count] # Divide the list of suspect pictures in half\n",
        "  second_half = prefed_set[half_count:] # This is the second half\n",
        "  if len(first_half) > 0:\n",
        "    files_created = DownloadCharacterAndUploadJsonRecursively(character, max_count, first_half) # Recrusive call 1 (Hope this list does not have picture causing problems)\n",
        "  if len(second_half) > 0:\n",
        "    files_created += DownloadCharacterAndUploadJsonRecursively(character, max_count, second_half) # Recursive call 2 (Hope htis list does not have picture causing problems)\n",
        "  return files_created # Return all valid pictures\n",
        "    \n",
        "\n",
        "max_count = 100\n",
        "\n",
        "while True:\n",
        "  for c in CHARACTERS:\n",
        "    if len(DownloadCharacterAndUploadJsonRecursively(c, max_count)) > max(max_count, 100) / 2:\n",
        "      gauth.Refresh()\n",
        "  time.sleep(60 * 5)\n",
        "  gauth.Refresh()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}